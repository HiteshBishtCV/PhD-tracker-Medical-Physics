name: Daily PhD Opportunities Scraper

on:
  schedule:
    # Run every day at 6:00 AM UTC
    - cron: '0 6 * * *'
  workflow_dispatch: # Allow manual triggering
  push:
    branches: [ main ]
    paths:
      - 'scraping/**'
      - '.github/workflows/daily-scrape.yml'

jobs:
  scrape-opportunities:
    runs-on: ubuntu-latest
    permissions:
      contents: write # Needed to commit back to repository
      issues: write   # Needed to create issues on errors
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('scraping/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
          
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r scraping/requirements.txt
        
    - name: Install Chrome and ChromeDriver
      run: |
        # Install Chrome
        wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
        sudo sh -c 'echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" >> /etc/apt/sources.list.d/google-chrome.list'
        sudo apt-get update
        sudo apt-get install -y google-chrome-stable
        
        # Install ChromeDriver
        CHROME_VERSION=$(google-chrome --version | cut -d " " -f3 | cut -d "." -f1)
        CHROMEDRIVER_VERSION=$(curl -s "https://chromedriver.storage.googleapis.com/LATEST_RELEASE_$CHROME_VERSION")
        wget -O /tmp/chromedriver.zip "https://chromedriver.storage.googleapis.com/chromedriver_linux64.zip"
        sudo unzip /tmp/chromedriver.zip chromedriver -d /usr/local/bin/
        sudo chmod +x /usr/local/bin/chromedriver
        
    - name: Create data directory
      run: |
        mkdir -p data/archive
        
    - name: Initialize sources file if not exists
      run: |
        if [ ! -f data/sources.json ]; then
          echo '[]' > data/sources.json
        fi
        
    - name: Run scraper
      env:
        PYTHONPATH: ${{ github.workspace }}
      run: |
        cd scraping
        python scraper.py --config ../data/sources.json --output ../data/opportunities.json --headless
        
    - name: Check for changes
      id: git-check
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        
        # Check if there are changes to commit
        if git diff --quiet; then
          echo "changes=false" >> $GITHUB_OUTPUT
        else
          echo "changes=true" >> $GITHUB_OUTPUT
        fi
        
    - name: Commit and push changes
      if: steps.git-check.outputs.changes == 'true'
      run: |
        git add data/
        git commit -m "ðŸ¤– Update PhD opportunities - $(date +'%Y-%m-%d %H:%M:%S UTC')"
        git push
        
    - name: Generate summary
      id: summary
      run: |
        if [ -f data/opportunities.json ]; then
          TOTAL_OPPS=$(python -c "import json; data=json.load(open('data/opportunities.json')); print(len(data))")
          echo "total_opportunities=$TOTAL_OPPS" >> $GITHUB_OUTPUT
        else
          echo "total_opportunities=0" >> $GITHUB_OUTPUT
        fi
        
    - name: Create success comment
      if: success()
      uses: actions/github-script@v7
      with:
        script: |
          const totalOpps = '${{ steps.summary.outputs.total_opportunities }}';
          const date = new Date().toISOString().split('T')[0];
          
          // You could create an issue comment or discussion here
          console.log(`âœ… Scraping completed successfully on ${date}`);
          console.log(`ðŸ“Š Total opportunities: ${totalOpps}`);
          
    - name: Handle scraping errors
      if: failure()
      uses: actions/github-script@v7
      with:
        script: |
          const title = `ðŸš¨ Daily scraping failed - ${new Date().toISOString().split('T')[0]}`;
          const body = `
          The daily PhD opportunities scraping job has failed.
          
          **Details:**
          - Workflow: ${context.workflow}
          - Run ID: ${context.runId}
          - Commit: ${context.sha}
          
          **Actions needed:**
          1. Check the workflow logs for errors
          2. Verify source websites are accessible
          3. Update selectors if websites have changed
          4. Check if any sources need to be disabled
          
          **Logs:** [View workflow run](${context.payload.repository.html_url}/actions/runs/${context.runId})
          `;
          
          // Create an issue for tracking
          await github.rest.issues.create({
            owner: context.repo.owner,
            repo: context.repo.repo,
            title: title,
            body: body,
            labels: ['scraping-error', 'automated']
          });
          
    - name: Upload artifacts
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: scraping-logs
        path: |
          scraping.log
          data/
        retention-days: 30

  deploy-to-github-pages:
    needs: scrape-opportunities
    if: always() # Run even if scraping partially fails
    runs-on: ubuntu-latest
    permissions:
      contents: read
      pages: write
      id-token: write
      
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
      
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        ref: main # Ensure we get the latest commit with data
        
    - name: Setup Pages
      uses: actions/configure-pages@v3
      
    - name: Upload to GitHub Pages
      uses: actions/upload-pages-artifact@v2
      with:
        path: '.'
        
    - name: Deploy to GitHub Pages
      id: deployment
      uses: actions/deploy-pages@v2

  # Weekly cleanup job
  weekly-cleanup:
    runs-on: ubuntu-latest
    if: github.event.schedule == '0 6 * * 0' # Only run on Sundays
    permissions:
      contents: write
      
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: Clean up old data
      run: |
        python -c "
        import json
        import os
        from datetime import datetime, timedelta
        
        # Clean up opportunities older than 1 year
        if os.path.exists('data/opportunities.json'):
            with open('data/opportunities.json', 'r') as f:
                opportunities = json.load(f)
            
            cutoff_date = (datetime.now() - timedelta(days=365)).strftime('%Y-%m-%d')
            
            # Keep only recent opportunities
            recent_opportunities = [
                opp for opp in opportunities 
                if opp.get('dateAdded', '9999-12-31') > cutoff_date
            ]
            
            with open('data/opportunities.json', 'w') as f:
                json.dump(recent_opportunities, f, indent=2)
            
            print(f'Cleaned up old opportunities. Kept {len(recent_opportunities)} recent ones.')
        "
        
    - name: Commit cleanup changes
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        
        if ! git diff --quiet; then
          git add data/
          git commit -m "ðŸ§¹ Weekly cleanup - Remove old opportunities"
          git push
        fi
